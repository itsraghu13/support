
"CAST(SUBSTRING_INDEX(duration, ' ', 1) AS INT) * 60 + " +
  "CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(duration, ' ', -2), ' ', 1) AS INT)"

def process_partition(partition, subscription_id, resource_group_name, factory_name, start_time, end_time, access_token):
    # Initialize Spark session
    spark = SparkSession.builder.getOrCreate()

    # Get pipeline runs for the partition
    pipeline_runs = []
    for item in partition:
        run_id = item['runId']
        pipeline_name = item.get('pipelineName', '')

        queryActivity_URL = f'https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/pipelineruns/{run_id}/queryActivityruns?api-version=2018-06-01&startTime={start_time}&endTime={end_time}&pipelineName={pipeline_name}'
        headers = {'Authorization': 'Bearer ' + access_token}

        # Get pipeline run details for given pipelineName and run id
        pipeline_run = get_pipeline_runs(queryActivity_URL, headers)
        if pipeline_run is not None:
            pipeline_runs.append(pipeline_run)

    # Process pipeline runs
    for pipeline_run in pipeline_runs:
        run_id = pipeline_run['runId']
        result = get_activity_output_data(pipeline_run, run_id)
        P_id = get_parent_id_recursive(run_id)

        if result:
            df = spark.createDataFrame(result, schema)

            if P_id:
                df = df.withColumn("Parent RunId", lit(P_id)).withColumn("requested date", lit(current_date()))
            else:
                df = df.withColumn("Parent RunId", lit('NA')).withColumn("requested date", lit(current_date()))

            df_parameter_data = get_parameters_data(run_id)

            if spark.catalog.tableExists("structured._meta_copyactivities"):
                df.createOrReplaceTempView('temp_copyactivities')
                sql_query_upsert = f'MERGE INTO structured._meta_copyactivities AS target USING temp_copyactivities AS source ON target.RunID = source.RunID WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *'
                spark.sql(sql_query_upsert)
            else:
                df.write.format("delta").mode("append").option("overwriteSchema", "true").saveAsTable('structured._meta_copyactivities')
        else:
            print("Result is empty for run_id:", run_id)

def main():
    subscription_id = "YOUR_SUBSCRIPTION_ID"
    resource_group_name = "YOUR_RESOURCE_GROUP_NAME"
    factory_name = "YOUR_FACTORY_NAME"
    start_time = "YOUR_START_TIME"
    end_time = "YOUR_END_TIME"
    access_token = "YOUR_ACCESS_TOKEN"

    if len(run_list) > 0:
        spark = SparkSession.builder.getOrCreate()
        sc = spark.sparkContext

        # Convert run_list to RDD
        run_rdd = sc.parallelize(run_list)

        # Process each partition in parallel
        run_rdd.foreachPartition(lambda partition: process_partition(partition, subscription_id, resource_group_name, factory_name, start_time, end_time, access_token))
    else:
        raise Exception('get_pipeline_list returned empty')

if __name__ == '__main__':
    main()








import multiprocessing

def process_item(item):
    subscription_id = "your_subscription_id"
    resource_GroupName = "your_resource_group_name"
    factory_name = "your_factory_name"
    start_time = "your_start_time"
    end_time = "your_end_time"
    pipeline_name = item['pipelineName']
    run_id = item['runId']

    queryActivity_URL = f'https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_GroupName}/providers/Microsoft.DataFactory/factories/{factory_name}/pipelineruns/{run_id}/queryActivityruns?api-version=2018-06-01&startTime={start_time}&endTime={end_time}&pipelineName={pipeline_name}'
    headers = {'Authorization': 'Bearer ' + access_token}

    pipeline_runs = get_pipeline_runs(queryActivity_URL, headers)

    if pipeline_runs is not None:
        result = get_activity_output_data(pipeline_runs, run_id)
        P_id = get_parent_id_recursive(run_id)

        if result:
            df = spark.createDataFrame(result, schema)

            if P_id:
                df = df.withColumn("Parent RunId", lit(P_id)).withColumn("requested date", lit(current_date()))
            else:
                df = df.withColumn("Parent_ RunId", lit('NA')).withColumn("requested date", lit(current_date()))
        else:
            return

        df_parmetes_data = get_parameters_data(run_id)

        if spark.catalog.tableExists("meta"):
            table_exists = True
        else:
            table_exists = False

        if table_exists:
            df.createOrReplaceTempView('temp_copyactivities')
            sql_query_upsert = f'MERGE INTO meta AS target USING temp_copyactivities AS source ON target.RunID = source.RunID WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *'
            spark.sql(sql_query_upsert)
        else:
            df.write.format("delta").mode("append").option("overwriteSchema", "true").saveAsTable('meta')
    else:
        raise Exception('get_pipeline_runs did not return anything')

if len(run_list) > 0:
    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())
    pool.map(process_item, run_list)
    pool.close()
    pool.join()
else:
    raise Exception('get_pipeline_list returned empty')













































https://learn.microsoft.com/en-us/answers/questions/569319/how-to-use-pagination-in-azure-data-factory-with-r

import requests

def fetch_pipeline_runs(url, headers):
    response = requests.post(url, headers=headers)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&lastUpdatedAfter={last_updated_after}&lastUpdatedBefore={last_updated_before}"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers)
    pipeline_runs = response.get('value', [])
    data = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs]

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        data.extend([{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs])
        continuation_token = response.get('continuationToken')

    return data

# Call the function
pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before)








import requests

def fetch_pipeline_runs(url, headers):
    response = requests.post(url, headers=headers)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers)
    pipeline_runs = response.get('value', [])
    data = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs]

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        filtered_runs = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['runStart'] <= end_time and start_time <= run['runEnd'] <= end_time]
        data.extend(filtered_runs)
        continuation_token = response.get('continuationToken')

    return data

# Call the function
pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time)










import requests
from concurrent.futures import ThreadPoolExecutor

def fetch_pipeline_runs(url, headers):
    response = requests.post(url, headers=headers)
    return response.json()

def process_continuation_token(url, headers, start_time, end_time):
    response = fetch_pipeline_runs(url, headers)
    pipeline_runs = response.get('value', [])
    filtered_runs = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['runStart'] <= end_time and start_time <= run['runEnd'] <= end_time]
    return filtered_runs, response.get('continuationToken')

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, batch_size=10, max_workers=10):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers)
    pipeline_runs = response.get('value', [])
    data = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs]

    continuation_tokens = [response.get('continuationToken')]

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        while continuation_tokens:
            batch_tokens = continuation_tokens[:batch_size]
            continuation_tokens = continuation_tokens[batch_size:]

            urls_with_tokens = [queryPipelineRuns_URL + f"&continuationToken={token}" for token in batch_tokens]
            
            futures = []
            for url_with_token in urls_with_tokens:
                future = executor.submit(process_continuation_token, url_with_token, headers, start_time, end_time)
                futures.append(future)

            for future in futures:
                filtered_runs, token = future.result()
                data.extend(filtered_runs)
                if token:
                    continuation_tokens.append(token)

    return data

# Call the function
pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time)







import requests

def fetch_pipeline_runs(url, headers):
    response = requests.post(url, headers=headers)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    continuation_token = None
    data = []

    while True:
        url = queryPipelineRuns_URL
        if continuation_token:
            url += f"&continuationToken={continuation_token}"

        response = fetch_pipeline_runs(url, headers)
        pipeline_runs = response.get('value', [])

        filtered_runs = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['runStart'] <= end_time and start_time <= run['runEnd'] <= end_time]
        data.extend(filtered_runs)

        continuation_token = response.get('continuationToken')
        if not continuation_token:
            break

    return data

# Call the function
pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time)










import requests

def fetch_pipeline_runs(url, headers):
    response = requests.get(url, headers=headers)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    continuation_token = None
    data = []

    while True:
        url = queryPipelineRuns_URL
        if continuation_token:
            url += f"&continuationToken={continuation_token}"

        response = fetch_pipeline_runs(url, headers)
        pipeline_runs = response.get('value', [])

        filtered_runs = [{'pipelineName': run['name'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['properties']['runStart'] <= end_time and start_time <= run['properties']['runEnd'] <= end_time]
        data.extend(filtered_runs)

        if 'continuationToken' in response:
            continuation_token = response['continuationToken']
        else:
            break

    return data

# Call the function
subscription_id = "your-subscription-id"
resource_group_name = "your-resource-group-name"
factory_name = "your-data-factory-name"
start_time = "2023-06-01T00:00:00Z"
end_time = "2023-06-30T23:59:59Z"
access_token = "your-access-token"

pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token)
print('Total pipeline runs retrieved:', len(pipeline_runs))
print(pipeline_runs)
















import requests

def fetch_pipeline_runs(url, headers):
    response = requests.get(url, headers=headers)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    continuation_token = None
    data = []

    while True:
        url = queryPipelineRuns_URL
        if continuation_token:
            url += f"&continuationToken={continuation_token}"

        response = fetch_pipeline_runs(url, headers)

        if response.status_code == 200:
            try:
                pipeline_runs = response.json().get('value', [])
                filtered_runs = [{'pipelineName': run['name'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['properties']['runStart'] <= end_time and start_time <= run['properties']['runEnd'] <= end_time]
                data.extend(filtered_runs)

                if 'continuationToken' in response.json():
                    continuation_token = response.json()['continuationToken']
                else:
                    break

            except ValueError as e:
                print('Error parsing JSON:', e)
                print('Response content:', response.content)
                break

        else:
            print('Error occurred while making the API call. Status code:', response.status_code)
            print('Response content:', response.content)
            break

    return data

# Call the function
subscription_id = "your-subscription-id"
resource_group_name = "your-resource-group-name"
factory_name = "your-data-factory-name"
start_time = "2023-06-01T00:00:00Z"
end_time = "2023-06-30T23:59:59Z"
access_token = "your-access-token"

pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token)
if pipeline_runs:
    print('Total pipeline runs retrieved:', len(pipeline_runs))
    print(pipeline_runs)














import requests

def fetch_pipeline_runs(url, headers):
    response = requests.get(url, headers=headers)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'&$top=100"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    offset = 0
    data = []

    while True:
        url = f"{queryPipelineRuns_URL}&$skip={offset}"

        response = fetch_pipeline_runs(url, headers)
        pipeline_runs = response.get('value', [])

        filtered_runs = [{'pipelineName': run['name'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['properties']['runStart'] <= end_time and start_time <= run['properties']['runEnd'] <= end_time]
        data.extend(filtered_runs)

        if len(pipeline_runs) < 100:
            break

        offset += 100

    return data

# Call the function
subscription_id = "your-subscription-id"
resource_group_name = "your-resource-group-name"
factory_name = "your-data-factory-name"
start_time = "2023-06-01T00:00:00Z"
end_time = "2023-06-30T23:59:59Z"
access_token = "your-access-token"

pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token)
print('Total pipeline runs retrieved:', len(pipeline_runs))
print(pipeline_runs)






















import requests

def fetch_pipeline_runs(url, headers):
    response = requests.post(url, headers=headers)
    return response.json()

def extract_continuation_token(continuation_token):
    # Extract the token value from the provided continuation token format
    token_start = continuation_token.find("'token': '") + len("'token': '")
    token_end = continuation_token.find("'", token_start)
    return continuation_token[token_start:token_end]

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01&$filter=runStart ge '{start_time}' and runEnd le '{end_time}'"
    
    headers = {'Authorization': 'Bearer ' + access_token}

    continuation_token = None
    data = []

    while True:
        url = queryPipelineRuns_URL
        if continuation_token:
            token = extract_continuation_token(continuation_token)
            url += f"&continuationToken={token}"

        response = fetch_pipeline_runs(url, headers)
        pipeline_runs = response.get('value', [])

        filtered_runs = [{'pipelineName': run['name'], 'runId': run['runId']} for run in pipeline_runs if start_time <= run['properties']['runStart'] <= end_time and start_time <= run['properties']['runEnd'] <= end_time]
        data.extend(filtered_runs)

        continuation_token = response.get('continuationToken')
        if not continuation_token:
            break

    return data

# Call the function
subscription_id = "your-subscription-id"
resource_group_name = "your-resource-group-name"
factory_name = "your-data-factory-name"
start_time = "2023-06-01T00:00:00Z"
end_time = "2023-06-30T23:59:59Z"
access_token = "your-access-token"

pipeline_runs = get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, start_time, end_time, access_token)
print('Total pipeline runs retrieved:', len(pipeline_runs))
print(pipeline_runs)

https://github.com/maxcarduner/spotify-recommender
















import requests

def fetch_pipeline_runs(url, headers, body=None):
    response = requests.post(url, headers=headers, json=body)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01"

    headers = {'Authorization': 'Bearer ' + access_token, 'Content-Type': 'application/json'}

    request_body = {
        'lastUpdatedAfter': last_updated_after,
        'lastUpdatedBefore': last_updated_before
    }

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers, body=request_body)
    pipeline_runs = response.get('value', [])
    data = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs]

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        data.extend([{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs])
        continuation_token = response.get('continuationToken')

    return data







request_body = {
        'lastUpdatedAfter': last_updated_after,
        'lastUpdatedBefore': last_updated_before,
        'filters': [
            {
                'operand': 'RunStart',
                'operator': 'Equals',
                'values': [last_updated_after]
            },
            {
                'operand': 'RunEnd',
                'operator': 'Equals',
                'values': [last_updated_before]
            }
        ]
    }


















import requests

def fetch_pipeline_runs(url, headers, body=None):
    response = requests.post(url, headers=headers, json=body)
    return response.json()

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01"

    headers = {'Authorization': 'Bearer ' + access_token, 'Content-Type': 'application/json'}

    filters = [
        {
            'operand': 'RunStart',
            'operator': 'GreaterThanOrEqual',
            'values': [last_updated_after]
        },
        {
            'operand': 'RunEnd',
            'operator': 'LessThanOrEqual',
            'values': [last_updated_before]
        }
    ]

    params = {
        'filters': filters
    }

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers, body=params)
    pipeline_runs = response.get('value', [])
    data = [{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs]

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        data.extend([{'pipelineName': run['pipelineName'], 'runId': run['runId']} for run in pipeline_runs])
        continuation_token = response.get('continuationToken')

    return data
















import requests

def fetch_pipeline_runs(url, headers):
    response = requests.get(url, headers=headers)
    return response.json()

def filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before):
    filtered_runs = []
    for run in pipeline_runs:
        run_start = run['runStart']
        run_end = run['runEnd']
        if last_updated_after <= run_start <= last_updated_before and last_updated_after <= run_end <= last_updated_before:
            filtered_runs.append({'pipelineName': run['pipelineName'], 'runId': run['runId']})
    return filtered_runs

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01"

    headers = {'Authorization': 'Bearer ' + access_token}

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers)
    pipeline_runs = response.get('value', [])

    filtered_runs = filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before)

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        filtered_runs.extend(filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before))
        continuation_token = response.get('continuationToken')

    return filtered_runs


























import requests

def fetch_pipeline_runs(url, headers):
    response = requests.get(url, headers=headers)
    return response.json()

def filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before):
    filtered_runs = []
    for run in pipeline_runs:
        run_start = run['properties']['runStart']
        run_end = run['properties']['runEnd']
        if last_updated_after <= run_end and last_updated_before >= run_start:
            filtered_runs.append({'pipelineName': run['name'], 'runId': run['runId']})
    return filtered_runs

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01"

    headers = {'Authorization': 'Bearer ' + access_token}

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers)
    pipeline_runs = response.get('value', [])

    filtered_runs = filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before)

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        filtered_runs.extend(filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before))
        continuation_token = response.get('continuationToken')

    return filtered_runs





















import requests

def fetch_pipeline_runs(url, headers, body=None):
    response = requests.post(url, headers=headers, json=body)
    return response.json()

def filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before):
    filtered_runs = []
    for run in pipeline_runs:
        run_start = run['runStart']
        run_end = run['runEnd']
        if last_updated_after <= run_end and last_updated_before >= run_start:
            filtered_runs.append({'pipelineName': run['pipelineName'], 'runId': run['runId']})
    return filtered_runs

def get_all_pipeline_runs(subscription_id, resource_group_name, factory_name, last_updated_after, last_updated_before, access_token):
    queryPipelineRuns_URL = f"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01"

    headers = {'Authorization': 'Bearer ' + access_token}

    request_body = {
        'lastUpdatedAfter': last_updated_after,
        'lastUpdatedBefore': last_updated_before,
        'filters': [
            {
                'operand': 'RunStart',
                'operator': 'Equals',
                'values': [last_updated_after]
            },
            {
                'operand': 'RunEnd',
                'operator': 'Equals',
                'values': [last_updated_before]
            }
        ]
    }

    response = fetch_pipeline_runs(queryPipelineRuns_URL, headers, body=request_body)
    pipeline_runs = response.get('value', [])

    filtered_runs = filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before)

    continuation_token = response.get('continuationToken')

    while continuation_token:
        url_with_token = queryPipelineRuns_URL + f"&continuationToken={continuation_token}"
        response = fetch_pipeline_runs(url_with_token, headers)
        pipeline_runs = response.get('value', [])
        filtered_runs.extend(filter_pipeline_runs(pipeline_runs, last_updated_after, last_updated_before))
        continuation_token = response.get('continuationToken')

    return filtered_runs




import re

data = """

"""

# Extract the continuation token using regular expressions
match = re.search(r'"token"\s*:\s*"(.+?)"', data)
if match:
    continuation_token = match.group(1)
    print("Continuation Token:", continuation_token)
else:
    print("Continuation Token not found in the data.")
























if len(run_list) > 0:
    headers = {'Authorization': 'Bearer ' + access_token}
    query_activity_uri = 'https://management.azure.com/subscriptions/(l/resourceGroups/(l/providers/Microsoft.DataFactory/factories/(/pipelineru'

    for run_item in run_list:
        pipeline_runs = get_pipeline_runs(query_activity_uri, headers)

        if pipeline_runs is not None:
            activity_result = get_activity_output_data(pipeline_runs, run_item['runId'])
            parent_id = get_parent_id_recursive(run_item['runId'])

            if activity_result:
                df = spark.createDataFrame(activity_result, schema)
                parent_run_id = parent_id if parent_id else 'NA'
                requested_date = current_date()

                df = df.withColumn("Parent RunId", lit(parent_run_id)).withColumn("Requested Date", lit(requested_date))

                parameters_data = get_parameters_data(run_item['runId'])

                if spark.catalog.tableExists("structured._meta_copyactivities"):
                    df.createOrReplaceTempView('temp_copyactivities')
                    sql_query_upsert = f'MERGE INTO structured._meta_copyactivities AS target USING temp_copyactivities AS source ON target.RunID = source.RunID WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *'
                    spark.sql(sql_query_upsert)
                else:
                    df.write.format("delta").mode("append").option("overwriteSchema", "true").saveAsTable('structured._meta_copyactivities')
        else:
            raise Exception('get_pipeline_runs did not return anything')
else:
    raise Exception('run_list is empty')
