SELECT AVG(
    CAST(SUBSTRING_INDEX(duration, ' min ', 1) AS INT) * 60 +
    CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(duration, ' min ', -1), ' sec', 1) AS INT)
  ) AS avg_duration_seconds
  FROM durations







WITH ActivityTrend AS (
    SELECT
        DATE(mc.request_date) AS activity_date,
        SUM(mc.rows_read) - SUM(mc.rows_written) AS total_rows_loaded
    FROM
        meta_copyactivity mc
    GROUP BY
        DATE(mc.request_date)
    HAVING
        SUM(mc.rows_read) - SUM(mc.rows_written) != 0
),
PipelineTrend AS (
    SELECT
        DATE(mp.start_time) AS pipeline_date,
        COUNT(*) AS pipeline_count
    FROM
        meta_pipelines mp
    GROUP BY
        DATE(mp.start_time)
)
SELECT
    a.activity_date,
    a.total_rows_loaded AS total_rows_loaded_from_activities,
    p.pipeline_date,
    p.pipeline_count
FROM
    ActivityTrend a
FULL OUTER JOIN
    PipelineTrend p ON a.activity_date = p.pipeline_date
ORDER BY
    COALESCE(a.activity_date, p.pipeline_date);
































WITH ActivityTrend AS (
    SELECT
        DATE(mc.request_date) AS activity_date,
        COUNT(*) AS activity_count
    FROM
        meta_copyactivity mc
    GROUP BY
        DATE(mc.request_date)
),
PipelineTrend AS (
    SELECT
        DATE(mp.start_time) AS pipeline_date,
        COUNT(*) AS pipeline_count
    FROM
        meta_pipelines mp
    GROUP BY
        DATE(mp.start_time)
),
DataLoadTrend AS (
    SELECT
        DATE(dl.process_load_date) AS load_date,
        SUM(dl.row_count) AS total_row_count_loaded
    FROM
        dataload dl
    GROUP BY
        DATE(dl.process_load_date)
)
SELECT
    a.activity_date,
    a.activity_count,
    p.pipeline_date,
    p.pipeline_count,
    d.load_date,
    d.total_row_count_loaded
FROM
    ActivityTrend a
FULL OUTER JOIN
    PipelineTrend p ON a.activity_date = p.pipeline_date
FULL OUTER JOIN
    DataLoadTrend d ON a.activity_date = d.load_date
WHERE
    d.total_row_count_loaded != 0
ORDER BY
    COALESCE(a.activity_date, p.pipeline_date, d.load_date);












SELECT meta_pipelines.pipeline_name, meta_pipelines.status, AVG(meta_copyactivity.rows_read) AS avg_rows_read, AVG(meta_copyactivity.data_read) AS avg_data_read, AVG(meta_copyactivity.rows_written) AS avg_rows_written, AVG(meta_copyactivity.data_written) AS avg_data_written FROM meta_pipelines INNER JOIN meta_copyactivity ON meta_pipelines.run_id = meta_copyactivity.parent_runid GROUP BY meta_pipelines.pipeline_name, meta_pipelines.status


WITH RankedData AS (
    SELECT
        mp.run_id AS pipeline_run_id,
        mp.start_time AS pipeline_start_time,
        mp.end_time AS pipeline_end_time,
        mc.activity_runid AS activity_run_id,
        mc.activity_name AS activity_name,
        dl.table_name AS loaded_table,
        SUM(dl.row_count) AS total_row_count_loaded,
        DENSE_RANK() OVER (ORDER BY SUM(dl.row_count) DESC) AS rank
    FROM
        meta_pipelines mp
    JOIN
        meta_copyactivity mc ON mp.run_id = mc.parent_runid
    LEFT JOIN
        dataload dl ON mc.activity_runid = dl.parent_runid
    GROUP BY
        mp.run_id,
        mp.start_time,
        mp.end_time,
        mc.activity_runid,
        mc.activity_name,
        dl.table_name
    HAVING
        SUM(dl.row_count) != 0
)
SELECT *
FROM RankedData
ORDER BY rank, pipeline_run_id, activity_run_id, loaded_table;
















from unidecode import unidecode

def remove_diacritics_and_replace(input_str):
    return unidecode(input_str)

input_string = "Joaé Ëx Âpple ñice Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)














import pyspark.sql.functions as F

def remove_diacritics_udf(input_str):
    return F.udf(lambda x: unidecode(x))(input_str)

schema = StructType([StructField('text', StringType(), True)])

spark_df = spark.createDataFrame(data=[['Joaé Ëxample Æpple ñicely Þorn']], schema=schema)

spark_df = spark_df.withColumn('text_no_diacritics', remove_diacritics_udf(spark_df['text']))

print(spark_df.show())














schema = StructType([StructField('text', StringType(), True)])

spark_df = spark.createDataFrame(data=[['Joaé Ëxample Æpple ñicely Þorn']], schema=schema)















import pyspark.sql.functions as F

def remove_diacritics_udf(input_str):
    return F.udf(lambda x: unidecode(x))(input_str)

spark_df = spark.createDataFrame({'text': ['Joaé Ëxample Æpple ñicely Þorn']})

spark_df = spark_df.withColumn('text_no_diacritics', remove_diacritics_udf(spark_df['text']))

print(spark_df.show())












from unidecode import unidecode

def remove_diacritics_and_replace(input_str):
    return unidecode(input_str)

input_string = "Joaé Ëxample Âpple ñicely Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)












import unicodedata

def remove_diacritics_and_replace(input_str):
    normalized_str = unicodedata.normalize('NFD', input_str)
    output_str = ''
    
    replacement_dict = {
        unicodedata.normalize('NFD', 'À'): 'A',
        unicodedata.normalize('NFD', 'Á'): 'A',
        unicodedata.normalize('NFD', 'Â'): 'A',
        unicodedata.normalize('NFD', 'Ã'): 'A',
        unicodedata.normalize('NFD', 'Ä'): 'A',
        unicodedata.normalize('NFD', 'Å'): 'A',
        unicodedata.normalize('NFD', 'à'): 'a',
        unicodedata.normalize('NFD', 'á'): 'a',
        unicodedata.normalize('NFD', 'â'): 'a',
        unicodedata.normalize('NFD', 'ã'): 'a',
        unicodedata.normalize('NFD', 'ä'): 'a',
        unicodedata.normalize('NFD', 'å'): 'a',
        unicodedata.normalize('NFD', 'È'): 'E',
        unicodedata.normalize('NFD', 'É'): 'E',
        # ... and so on for other characters in the dictionary
    }
    
    for c in normalized_str:
        output_str += replacement_dict.get(c, c)
    
    return output_str

input_string = "Joaé Ëxample Âpple ñicely Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)






















def remove_diacritics_and_replace(input_str):
    normalized_str = unicodedata.normalize('NFD', input_str)
    output_str = ''
    
    replacement_dict = {
        'À': 'A', 'Á': 'A', 'Â': 'A', 'Ã': 'A', 'Ä': 'A', 'Å': 'A',
        'à': 'a', 'á': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a', 'å': 'a',
        'È': 'E', 'É': 'E', 'Ê': 'E', 'Ë': 'E',
        'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e',
        'Ì': 'I', 'Í': 'I', 'Î': 'I', 'Ï': 'I',
        'ì': 'i', 'í': 'i', 'î': 'i', 'ï': 'i',
        'Ò': 'O', 'Ó': 'O', 'Ô': 'O', 'Õ': 'O', 'Ö': 'O', 'Ø': 'O',
        'ò': 'o', 'ó': 'o', 'ô': 'o', 'õ': 'o', 'ö': 'o', 'ø': 'o',
        'Ù': 'U', 'Ú': 'U', 'Û': 'U', 'Ü': 'U',
        'ù': 'u', 'ú': 'u', 'û': 'u', 'ü': 'u',
        'Ý': 'Y', 'ý': 'y', 'ÿ': 'y',
        'Ç': 'C', 'ç': 'c',
        'Ñ': 'N', 'ñ': 'n',
        'Æ': 'AE', 'æ': 'ae',
        'Þ': 'Th', 'þ': 'th', 'Ð': 'D', 'ð': 'd',  # Icelandic characters
        'ß': 'ss',
        'œ': 'oe', 'Œ': 'OE',
        'Ā': 'A', 'ā': 'a', 'Ă': 'A', 'ă': 'a', 'Ą': 'A', 'ą': 'a',  # Latin extended-A
        'Ć': 'C', 'ć': 'c', 'Ĉ': 'C', 'ĉ': 'c', 'Ċ': 'C', 'ċ': 'c',
    }
    
    for c in normalized_str:
        output_str += replacement_dict.get(c, c)
    
    return output_str

input_string = "Joaé Ëxample Âpple ñicely Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)

