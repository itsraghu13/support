import time
import random

def concurrent_dataload_table(query, retry=5):
    if retry > 1:
        try:
            return spark.sql(query)
        except Exception as e:
            retry -= 1
            delay = random.randrange(240, 301)
            time.sleep(delay)
            print(str(retry) + " Failed, added delay " + str(delay))
            return concurrent_dataload_table(query, retry)
    else:
        raise Exception("Update failed")

def insert_unique_record(configId):
    # Check if the record already exists
    existing_record = spark.sql("""
    SELECT status FROM structured.t_dataload WHERE config_id = {}
    """.format(configId)).collect()

    if not existing_record or existing_record[0].status != "running":
        # Record does not exist or status is not "running", proceed with insertion
        query = """
        SELECT config.config_id,
            coalesce(CONCAT('ID', config.config_id, 'R', max(cast(replace(process_id, CONCAT('ID', config.config_id, 'R'), '') as int)) + 1),
                    concat('ID', config.config_id, 'R', '1')) as process_id,
            CASE
                WHEN MAX(CASE WHEN dataload.status = 'running' THEN 1 ELSE 0 END) > 1 THEN 'duplicate run'
                ELSE 'running'
            END AS status
        FROM structured.t_database_config config
        LEFT JOIN structured.t_dataload dataload ON config.config_id = dataload.config_id
        WHERE config.config_id = {}
        GROUP BY config.config_id
        """.format(configId)

        result = concurrent_dataload_table(query).collect()
        entry_type = result[0][2]
        print(entry_type)

        if entry_type != "duplicate run":
            generated_process_id = result[0][1]
            insert_query = """
            INSERT INTO structured.t_dataload (config_id, process_id, status)
            VALUES ({}, '{}', 'running')
            """.format(configId, generated_process_id)
            spark.sql(insert_query)
        else:
            raise Exception("Duplicate Entry Found")

if createProcessId.upper() == "YES":
    insert_unique_record(configId)















































import time
import random

def concurrent_dataload_table(query, retry=5):
    if retry > 1:
        try:
            return spark.sql(query)
        except Exception as e:
            retry -= 1
            delay = random.randrange(240, 301)
            time.sleep(delay)
            print(str(retry) + " Failed, added delay " + str(delay))
            return concurrent_dataload_table(query, retry)
    else:
        raise Exception("Update failed")

def insert_unique_record(configId):
    query = """
    SELECT config.config_id,
        coalesce(CONCAT('ID', config.config_id, 'R', max(cast(replace(process_id, CONCAT('ID', config.config_id, 'R'), '') as int)) + 1),
                concat('ID', config.config_id, 'R', '1')) as process_id,
        CASE
            WHEN MAX(CASE WHEN dataload.status = 'running' THEN 1 ELSE 0 END) > 1 THEN 'duplicate run'
            ELSE 'running'
        END AS status
    FROM structured.t_database_config config
    LEFT JOIN structured.t_dataload dataload ON config.config_id = dataload.config_id
    WHERE config.config_id = {0}
    GROUP BY config.config_id
    """.format(configId)

    result = concurrent_dataload_table(query).collect()
    entry_type = result[0][2]
    print(entry_type)

    if result and entry_type != "duplicate run":
        generated_process_id = result[0][1]
        insert_query = """
        INSERT INTO structured.t_dataload (config_id, process_id, status)
        SELECT {0}, '{1}', 'running'
        WHERE NOT EXISTS (
            SELECT 1 FROM structured.t_dataload WHERE config_id = {0}
        )
        """.format(configId, generated_process_id)
        spark.sql(insert_query)
    else:
        raise Exception("Duplicate Entry Found")

if createProcessId.upper() == "YES":
    insert_unique_record(configId)




































def update_delta_table(data):
  for attempt in range(max_attempts):
    try:
      # Your code to update the Delta table with data
      break  # Successful update, exit the loop
    except (FileAlreadyExistsException, OptimisticTransactionConflictException) as e:
      if attempt >= max_attempts - 1:
        raise  # Reached maximum retries, re-raise the exception
      else:
        wait_time = calculate_backoff_time(attempt)
        print(f"Concurrent update detected. Retrying in {wait_time} seconds...")
        time.sleep(wait_time)

def calculate_backoff_time(attempt):
  base_wait_time = 2  # Initial wait time in seconds
  return base_wait_time * 2**attempt



















import org.apache.spark.sql.functions._

// Assuming your DataFrame is named df and the column is named original_date_column
val formattedDF = df.withColumn("formatted_date", from_unixtime(unix_timestamp(col("original_date_column"), "yyyyMMddHHmm")))

formattedDF.show()

-- Assuming your table is named my_table and the column is named original_date_column
SELECT 
  from_unixtime(cast(original_date_column as bigint) / 1000, 'yyyy-MM-dd HH:mm') AS formatted_date
FROM my_table;





-- Assuming your table is named my_table and the column is named original_date_column
SELECT 
  TO_DATE(from_unixtime(unix_timestamp(original_date_column, 'yyyyMMddHHmm')), 'yyyy-MM-dd') AS formatted_date
FROM my_table;



// Assuming your DataFrame is named df and the column is named original_number_column
df.createOrReplaceTempView("my_table")

// Run the SQL query
val modifiedDF = spark.sql("SELECT substr(cast(original_number_column as string), 1, length(cast(original_number_column as string)) - 4) as modified_number FROM my_table")

modifiedDF.show()

from datetime import datetime
import pytz

def convert_gmt_to_cst(gmt_timestamp):
    # Define timezone objects
    gmt_timezone = pytz.timezone('GMT')
    cst_timezone = pytz.timezone('America/Chicago')  # CST with daylight saving time adjustments

    # Convert the input timestamp from GMT to CST
    gmt_datetime = gmt_timezone.localize(gmt_timestamp)
    cst_datetime = gmt_datetime.astimezone(cst_timezone)

    return cst_datetime

# Example usage:
gmt_timestamp = datetime(2022, 1, 31, 12, 0, 0)  # Replace this with your GMT timestamp
cst_timestamp = convert_gmt_to_cst(gmt_timestamp)

print("GMT Timestamp:", gmt_timestamp)
print("CST Timestamp:", cst_timestamp)























from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("TableList").getOrCreate()

# Execute the SQL query
query_result = spark.sql("SELECT table_name FROM your_table")

# Collect the results into a list
table_names_list = [row["table_name"] for row in query_result.collect()]

# Print or use the list as needed
print(table_names_list)

# Stop the Spark session
spark.stop()




















from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("TableDeletion").getOrCreate()

# Define your table names
table_names = ["table1", "table2", "table3", "table4", "table5"]

# Iterate through each table and delete all rows
for table_name in table_names:
    delete_query = f"DELETE FROM {table_name}"
    
    # Execute the delete query
    spark.sql(delete_query)

# Stop the Spark session
spark.stop()










def extract_table_name(input_string, prefix_to_remove, suffix_to_remove):
    # Remove prefix
    if input_string.startswith(prefix_to_remove):
        input_string = input_string[len(prefix_to_remove):]

    # Remove suffix
    if input_string.endswith(suffix_to_remove):
        input_string = input_string[:-len(suffix_to_remove)]

    return input_string

if __name__ == "__main__":
    # Replace 'input_string', 'prefix_to_remove', and 'suffix_to_remove' with your actual values
    input_string = "t_facets_AB_table_name_cdc"
    prefix_to_remove = "t_facets_AB"
    suffix_to_remove = "_cdc"

    # Extract the table name
    table_name = extract_table_name(input_string, prefix_to_remove, suffix_to_remove)

    print("Table name:", table_name)






















from pyspark.sql import SparkSession

def search_tables_for_string(spark, database_name, search_string):
    try:
        # Show tables in the specified database
        tables = spark.sql(f"SHOW TABLES IN {database_name}")

        # Iterate through tables and search for the string in table names
        matching_tables = [row['tableName'] for row in tables.collect() if search_string in row['tableName']]

        # Display matching tables
        if matching_tables:
            print("Matching tables:")
            for table_name in matching_tables:
                print(table_name)
        else:
            print("No matching tables found.")

        print("Search completed.")
    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    # Create a Spark session
    spark = SparkSession.builder.appName("table_search").getOrCreate()

    # Replace 'your_database' and 'your_search_string' with your actual database and search string
    database_name = 'your_database'
    search_string = 'your_search_string'

    # Call the function to search for the string in table names
   













SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(column_name, '_', -2), '_', 1) AS table_name
FROM your_table;





SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(column1, '_', 3), '_', -1) AS table_name
FROM my_table;

SELECT REPLACE(REPLACE(column1, '_cdc', ''), '_', '') AS table_name
FROM my_table;


SELECT SUBSTRING(
   SUBSTRING(columnName, POSITION('_', columnName) + 1),
   1,
   POSITION('_', SUBSTRING(columnName, POSITION('_', columnName) + 1)) - 1
) AS TableName
FROM yourTable;



SELECT REGEXP_SUBSTR(columnName, '_([^_]+)_cdc', 2) AS TableName
FROM yourTable;


SELECT REGEXP_SUBSTR(columnName, '_([^_]+)_(?!cdc)', 1, 1, NULL, 1) AS TableName
FROM yourTable;
































SELECT
  SUBSTRING('{0}', POSITION('_', '{0}') + LENGTH('_source_name_'), LENGTH('{0}') - POSITION('_', '{0}') - POSITION('*', '{0}') - LENGTH('_source_name_') - POSITION('*', REVERSE('{0}'))) as TableName;

SELECT
  REGEXP_REPLACE(table_name, '^t_([^_]+)_.*_([^_]+)_.*_cdc$', '$2') AS modified_table_name
FROM
  structured.t_database_config
WHERE
  table_name LIKE 't\_%\_cdc'
  AND table_name = '*+' || REPLACE(REPLACE(merge_col_tableName, 't_', ''), '_cdc', '') + '*'
  AND config_id = '123';


SELECT
  REGEXP_REPLACE(table_name, '^t_([^_]+)_([^_]+)_.*_cdc$', '$2') AS modified_table_name
FROM
  structured.t_database_config
WHERE
  table_name LIKE 't\_%\_cdc'
  AND table_name = '*+' || REPLACE(REPLACE(merge_col_tableName, 't_', ''), '_cdc', '') + '*'
  AND config_id = '123';




SELECT
    SUBSTRING(table_name, 3, CHARINDEX('_cdc', table_name) - 3) AS modified_table_name
FROM
    your_database_name.information_schema.tables
WHERE
    table_name LIKE 't\_%\_cdc' ESCAPE '\';

    SELECT
  REGEXP_REPLACE(table_name, 't_([^_]+)_([^_]+)_cdc', '') AS modified_table_name
FROM
  structured.t_database_config
WHERE
  table_name LIKE 't\_%\_cdc'
  AND table_name = '*+' || REPLACE(REPLACE(merge_col_tableName, 't_', ''), '_cdc', '') + '*'
  AND config_id = '123';



  SELECT
  REGEXP_REPLACE(table_name, 't_([^_]+)_([^_]+)_([^_]+)', '$2') AS modified_table_name
FROM
  structured.t_database_config
WHERE
  table_name LIKE 't\_%\_cdc'
  AND table_name = '*+' || REPLACE(REPLACE(merge_col_tableName, 't_', ''), '_cdc', '') + '*'
  AND config_id = '123';





    SELECT
  REGEXP_REPLACE(table_name, 't_([^_]+)_([^_]+)_', '') AS modified_table_name
FROM
  structured.t_database_config
WHERE
  table_name LIKE 't\_%\_cdc'
  AND table_name = '*+' || REPLACE(REPLACE(merge_col_tableName, 't_', ''), '_cdc', '') + '*'
  AND config_id = '123';




SELECT
    CONCAT('t_', SUBSTRING(table_name, CHARINDEX('_source_name_', table_name) + LEN('_source_name_'), LEN(table_name))) AS modified_table_name
FROM
    your_database_name.information_schema.tables
WHERE
    table_name LIKE 't\_source_name\_%' ESCAPE '\';

import re

# Assuming merge_col_tableName is a string variable containing the table name
merge_col_tableName = "*+merge_col_tableName.replace('t_', '').replace('_cdc', '')+'"

# Use regular expression to remove dynamic source_name
merge_col_tableName = re.sub(r'_\w+', '', merge_col_tableName)


SELECT
  REGEXP_REPLACE(table_name, 't_([^_]+)_', '') AS modified_table_name
FROM
  structured.t_database_config
WHERE
  table_name LIKE 't\_%\_cdc'
  AND table_name = '*+' || REPLACE(REPLACE(merge_col_tableName, 't_', ''), '_cdc', '') + '*'
  AND config_id = ' + ConfigId + '*


result = spark.sql('select merge_columns from structured.t_database_config where table_name=' + merge_col_tableName + ' and config_id=' + ConfigId).collect()[0][0]
















from pyspark.sql.functions import col, lit, row_number
from pyspark.sql.window import Window

# Assuming 'structured' is your database name, and 't_database_config' is your table name
df_count = spark.sql('select count(*) as count from structured.t_database_config').collect()[0]['count']

# Using row_number() function and a window specification
window_spec = Window.orderBy("table_list")
df = spark.sql("""
    SELECT
        (ROW_NUMBER() OVER (ORDER BY table_list) + {count}) as config_id
    FROM
        structured.t_stg_cdc_config
    WHERE
        load_date > (SELECT MAX(load_date) FROM structured.t_st_cdc_config) * 2
""").replace('NULL', None)

df.createOrReplaceTempView("tempDF")






















import datetime

# ... (previous code)

# Adjusted calculation (example, adjust as needed)
nanoseconds = (lsn_value + 9223372036854775808) * 100  # Handle potential negative value
seconds = nanoseconds / 1e9

# Add difference if LSN epoch is not Unix epoch (example, adjust as needed)
epoch_difference = 1577836800  # Assuming LSN epoch is January 1, 2020
seconds += epoch_difference

# Create datetime object
timestamp = datetime.datetime.fromtimestamp(seconds, tz=datetime.timezone.utc)

print(timestamp)







import datetime

lsn_hex = '0x000D79AC00C992D80011'
lsn_int = int(lsn_hex, 16)

# Extract relevant portion (assuming lower 32 bits)
lsn_value = lsn_int & 0x00000000FFFFFFFF

# Calculate nanoseconds and seconds (adjust base value if needed)
nanoseconds = (lsn_value - 0xD79AC00C992D80000) * 100
seconds = nanoseconds / 1e9

# Create datetime object, considering time zone if applicable
timestamp = datetime.datetime.fromtimestamp(seconds, tz=datetime.timezone.utc)  # Assuming UTC

print(timestamp)












import datetime

# Example LSN value
lsn_hex = '0x000D79AC00C992D80011'

# Convert hexadecimal to integer
lsn_int = int(lsn_hex, 16)

# Convert LSN to nanoseconds since epoch
nanoseconds = (lsn_int - 0xD79AC00C992D80000) * 100

# Convert nanoseconds to seconds since epoch
seconds = nanoseconds / 1e9

# Create a datetime object from the seconds
timestamp = datetime.datetime.utcfromtimestamp(seconds)

# Print the timestamp
print(timestamp)


def lsn_to_datetime(lsn):
    # ... (previous code)

    # Calculate days and seconds separately
    days_since_base_date = seconds_since_base_date // (24 * 60 * 60)
    seconds_in_day = seconds_since_base_date % (24 * 60 * 60)

    # Add days and seconds incrementally
    timestamp = base_date
    for _ in range(days_since_base_date):
        timestamp += timedelta(days=1)
    timestamp += timedelta(seconds=seconds_in_day)

    return timestamp













https://lucavallarelli.altervista.org/blog/service-principal-authentication-data-factory/


import datetime

# Example lsn value
lsn = 1671532800000000000

# Convert lsn to nanoseconds since epoch
nanoseconds = lsn // 100

# Convert nanoseconds to seconds since epoch
seconds = nanoseconds / 1e9

# Create a datetime object from the seconds
timestamp = datetime.datetime.utcfromtimestamp(seconds)

# Print the timestamp
print(timestamp)



import struct
from datetime import datetime, timedelta

def lsn_to_datetime(lsn):
    # SQL Server LSN structure: 10 bytes (8 bytes for LSN, 2 bytes for unused)
    lsn_bytes = bytes.fromhex(lsn)
    
    # Extract the LSN components
    lsn_value, unused = struct.unpack('<QH', lsn_bytes)
    
    # Calculate the timestamp based on LSN components
    base_date = datetime(1900, 1, 1)
    seconds_since_base_date = lsn_value / 10000000  # LSN value is in 100-nanosecond intervals
    timestamp = base_date + timedelta(seconds=seconds_since_base_date)
    
    return timestamp

# Example usage
lsn_value = "0x000D79AC00C992D80011"
timestamp = lsn_to_datetime(lsn_value)

print(f"Converted Timestamp: {timestamp}")




import struct
from datetime import datetime, timedelta

def lsn_to_datetime(lsn):
    # Convert the hexadecimal string to bytes
    lsn_bytes = bytes.fromhex(lsn[2:])  # Remove the '0x' prefix from the LSN
    
    # Extract the LSN components
    lsn_value, unused = struct.unpack('<QH', lsn_bytes)
    
    # Calculate the timestamp based on LSN components
    base_date = datetime(1900, 1, 1)
    seconds_since_base_date = lsn_value / 10000000  # LSN value is in 100-nanosecond intervals
    
    try:
        timestamp = base_date + timedelta(seconds=seconds_since_base_date)
        return timestamp
    except OverflowError as e:
        print(f"Error: {e}")
        print(f"LSN value: {lsn_value}")
        print(f"Seconds since base date: {seconds_since_base_date}")
        raise

# Example usage
lsn_value = "0x000D79AC00C992D80011"
timestamp = lsn_to_datetime(lsn_value)

print(f"Converted Timestamp: {timestamp}")






SELECT
    config.config_id,
    CONCAT('SELECT * FROM ', config.schema_name, '.', REPLACE(config.table_name, 'dbo.', '')) AS query,
    DATE_FORMAT(delta_end_time, 'yyyy/MM/dd HH:mm:ss') AS begin_time,
    DATE_FORMAT(delta_end_time, 'yyyy/MM/dd HH:mm:ss') AS end_time,
    config.table_name,
    config.load_type,
    config.schema_name,
    config.do_name,
    config.source_name,
    config.db_Type,
    config.frequency,
    config.secret_name,
    config.watermark_Columns,
    config.custom_refresh,
    dataload.process_id
FROM
    structured.t_database_config config
LEFT JOIN (
    SELECT
        t_dataload.config_id,
        MAX(CAST(REPLACE(t_dataload.process_id, 'ID', '') AS INT)) AS max_process_id
    FROM
        structured.t_dataload t_dataload
    WHERE
        t_dataload.status = 'success'
    GROUP BY
        t_dataload.config_id
) max_processid ON config.config_id = max_processid.config_id
LEFT JOIN structured.t_dataload dataload ON config.config_id = dataload.config_id
    AND CAST(REPLACE(dataload.process_id, 'ID', '') AS INT) = max_processid.max_process_id
WHERE
    config.do_name = 'int1'
    AND config.author = 'Raghav'
    AND config.db_Type = 'sql server';



















Sure, I can help you with that. Here is the formatted and corrected text:

Hi team,

Good morning! My name is Raghavendra.

Log Scrapping

We are moving from use case-based pipelines to config-based pipelines. This will make it difficult to know what table was loaded by which pipeline. Currently, we have to search the entire log monitor in ADF, which is time-consuming and inefficient. Additionally, including a logging mechanism in the pipeline will take more time and increase the pipeline size.

To simplify this, we can use the ADF REST API approach, which will allow us to capture the data passively.

Solution

By using the ADF REST API approach, we can passively log all ingestion, curation, and data extraction to a log scraper. This will help us with data validation, load monitoring, debugging, and reporting.







WITH CTE AS (
  SELECT
    tmc.activity_name,
    tmc.parent_name,
    tmp.pipeline_name,
    tmp.duration,
    tmc.rows_read,
    CAST(SUBSTRING_INDEX(tmp.duration, ' min', 1) AS DECIMAL) +
    CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(tmp.duration, ' min', -1), ' sec', 1) AS DECIMAL) / 60 AS duration_in_hours
  FROM structured.t_copy tmc
  INNER JOIN structured.t__pipe tmp ON tmc.parent_runid = tmp.run_id
)
SELECT
  activity_name,
  parent_name,
  pipeline_name,
  COUNT(*) AS total_runs,
  ROUND(AVG(duration_in_hours), 2) AS avg_duration,
  ROUND(AVG(rows_read), 2) AS avg_rows,
  AVG(rows_read) OVER (PARTITION BY pipeline_name ORDER BY CAST(tmp.start_time AS DATE) ROWS BETWEEN 14 PRECEDING AND CURRENT ROW) AS avg_runtime_15,
  AVG(rows_read) OVER (PARTITION BY pipeline_name ORDER BY CAST(tmp.start_time AS DATE) ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS avg_runtime_30
FROM CTE
GROUP BY activity_name, parent_name, pipeline_name;


































r'/\/\/|"\/"|\/\|'

r'/\/\/|"\/"|\/\|'


r"/{2}"\/"{2}\|"

r'/(//|\\"|\\|)'


SELECT
    TRANSFORM(
        SPLIT(data_column, '\\|'),
        x -> MAP(
            SPLIT(x, ':')[0],  -- Key
            SPLIT(x, ':')[1]   -- Value
        )
    ) AS json_data
FROM your_table;


SELECT
  JSON_OBJECT(key, value) AS json_data
FROM (
  SELECT
    SPLIT(data_column, '|') AS row,
    SPLIT(row, ':')[0] AS key,
    SPLIT(row, ':')[1] AS value
  FROM your_table
)
WHERE
  key != 0













  

SELECT
    TRANSFORM(
        SPLIT(data_column, '|'),
        x -> STR_TO_MAP(x, ':', ',')
    ) AS json_data
FROM your_table;


SELECT
    TRANSFORM(
        SPLIT(data_column, '|'),
        x -> STR_TO_MAP(x, ':', ',')
    ) AS json_data
FROM your_table;




SELECT CONCAT('{ "data": [', REGEXP_REPLACE(data_column, '\\|', ', '), '] }') AS json_data
FROM your_table;








/|\/\/|/\\|

r'/(?:/|\|)'

r'/\/\||\/{2}|\/'



r'//.*"/".*/\|'


pattern1 = r'//'  # Double forward slash
pattern2 = r'"\/"'  # Double quote, forward slash, double quote
pattern3 = r'\|/'  # Vertical bar followed by a forward slash

r'/\/(\/|\|)?'
r'/\/(\/|\\|)?'
x

r"^\\b//\\b"


r".*//.*"


r'/\/{1,2}|\|\/|\/\|'


/\/\/|"[\\/]"|\|\\/


"/".*//.*/\|













SELECT
  meta_pipelines.pipeline_name,
  COUNT(*) AS total_run,
  AVG(meta_copyactivity.rows_processed) AS avg_rows,
  AVG(meta_copyactivity.rows_processed) OVER (PARTITION BY meta_pipelines.pipeline_name ORDER BY meta_copyactivity.request_date ROWS BETWEEN 14 PRECEDING AND CURRENT ROW) AS avg_rows_15,
  AVG(meta_copyactivity.rows_processed) OVER (PARTITION BY meta_pipelines.pipeline_name ORDER BY meta_copyactivity.request_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS avg_rows_30,
  AVG(meta_copyactivity.duration) OVER (PARTITION BY meta_pipelines.pipeline_name) AS avg_duration
FROM meta_copyactivity
INNER JOIN meta_pipelines ON meta_copyactivity.parent_runid = meta_pipelines.run_id
GROUP BY meta_pipelines.pipeline_name;










SELECT
  tmc.activity_name,
  tmc.parent_name,
  tmp.pipeline_name,
  COUNT(*) AS total_runs,
  ROUND(
    CAST(SUBSTRING_INDEX(tmp.duration, ' min', 1) AS DECIMAL) +
    CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(tmp.duration, ' min', -1), ' sec', 1) AS DECIMAL) / 60
  ) AS avg_duration,
  AVG(tmc.rows_read) AS avg_rows,
  AVG(tmc.rows_read) OVER (PARTITION BY tmp.pipeline_name ORDER BY CAST(tmp.start_time AS DATE) ROWS BETWEEN 14 PRECEDING AND CURRENT ROW) AS avg_runtime_15,
  AVG(tmc.rows_read) OVER (PARTITION BY tmp.pipeline_name ORDER BY CAST(tmp.start_time AS DATE) ROWS BETWEEN 29 PRECEDING AND CURRENT ROW) AS avg_runtime_30
FROM structured.t_copy tmc
INNER JOIN structured.t__pipe tmp ON tmc.parent_runid = tmp.run_id
GROUP BY tmc.activity_name, tmc.parent_name, tmp.pipeline_name;

















SELECT
  meta_pipelines.pipeline_name,
  CONCAT(COUNT(CASE WHEN meta_copyactivity.status = 'SUCCESS' THEN 1 END), ', ', COUNT(CASE WHEN meta_copyactivity.status = 'FAILURE' THEN 1 END)) AS total_count,
  meta_copyactivity.start_time
FROM meta_copyactivity
INNER JOIN meta_pipelines ON meta_copyactivity.parent_runid = meta_pipelines.run_id
GROUP BY meta_pipelines.pipeline_name, meta_copyactivity.start_time;









SELECT
    meta_pipelines.pipeline_name,
    AVG(meta_copyactivity.duration) AS avg_duration,
    SUM(meta_copyactivity.rows_processed) / COUNT(*) AS avg_rows,
    MAX(meta_copyactivity.duration) AS max_duration,
    avg_rows_15
FROM meta_copyactivity
INNER JOIN meta_pipelines ON meta_copyactivity.parent_runid = meta_pipelines.run_id
INNER JOIN (
    SELECT
        meta_pipelines.pipeline_name,
        AVG(meta_copyactivity.rows_processed) AS avg_rows_15
    FROM
        meta_copyactivity
    INNER JOIN meta_pipelines ON meta_copyactivity.parent_runid = meta_pipelines.run_id
    GROUP BY
        meta_pipelines.pipeline_name
) AS subquery ON meta_pipelines.pipeline_name = subquery.pipeline_name
GROUP BY
    meta_pipelines.pipeline_name, avg_rows_15;

















SELECT meta_pipelines.pipeline_name,
       AVG(meta_copyactivity.duration) AS avg_duration,
       SUM(meta_copyactivity.rows_processed) / COUNT(*) AS avg_rows,
       MAX(meta_copyactivity.duration) AS max_duration,
       AVG(meta_copyactivity.rows_processed) OVER (PARTITION BY meta_pipelines.pipeline_name ORDER BY meta_copyactivity.request_date ROWS BETWEEN 14 PRECEDING AND CURRENT ROW) AS avg_rows_15
FROM meta_copyactivity
INNER JOIN meta_pipelines ON meta_copyactivity.parent_runid = meta_pipelines.run_id
GROUP BY meta_pipelines.pipeline_name;




SELECT AVG(
    CAST(SUBSTRING_INDEX(duration, ' min ', 1) AS INT) * 60 +
    CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(duration, ' min ', -1), ' sec', 1) AS INT)
  ) AS avg_duration_seconds
  FROM durations


SELECT AVG(
    CAST(SUBSTRING_INDEX(duration, ' min ', 1) AS DECIMAL) +
    CAST(SUBSTRING_INDEX(SUBSTRING_INDEX(duration, ' min ', -1), ' sec', 1) AS DECIMAL) / 60
  ) AS avg_duration_minutes







WITH ActivityTrend AS (
    SELECT
        DATE(mc.request_date) AS activity_date,
        SUM(mc.rows_read) - SUM(mc.rows_written) AS total_rows_loaded
    FROM
        meta_copyactivity mc
    GROUP BY
        DATE(mc.request_date)
    HAVING
        SUM(mc.rows_read) - SUM(mc.rows_written) != 0
),
PipelineTrend AS (
    SELECT
        DATE(mp.start_time) AS pipeline_date,
        COUNT(*) AS pipeline_count
    FROM
        meta_pipelines mp
    GROUP BY
        DATE(mp.start_time)
)
SELECT
    a.activity_date,
    a.total_rows_loaded AS total_rows_loaded_from_activities,
    p.pipeline_date,
    p.pipeline_count
FROM
    ActivityTrend a
FULL OUTER JOIN
    PipelineTrend p ON a.activity_date = p.pipeline_date
ORDER BY
    COALESCE(a.activity_date, p.pipeline_date);
































WITH ActivityTrend AS (
    SELECT
        DATE(mc.request_date) AS activity_date,
        COUNT(*) AS activity_count
    FROM
        meta_copyactivity mc
    GROUP BY
        DATE(mc.request_date)
),
PipelineTrend AS (
    SELECT
        DATE(mp.start_time) AS pipeline_date,
        COUNT(*) AS pipeline_count
    FROM
        meta_pipelines mp
    GROUP BY
        DATE(mp.start_time)
),
DataLoadTrend AS (
    SELECT
        DATE(dl.process_load_date) AS load_date,
        SUM(dl.row_count) AS total_row_count_loaded
    FROM
        dataload dl
    GROUP BY
        DATE(dl.process_load_date)
)
SELECT
    a.activity_date,
    a.activity_count,
    p.pipeline_date,
    p.pipeline_count,
    d.load_date,
    d.total_row_count_loaded
FROM
    ActivityTrend a
FULL OUTER JOIN
    PipelineTrend p ON a.activity_date = p.pipeline_date
FULL OUTER JOIN
    DataLoadTrend d ON a.activity_date = d.load_date
WHERE
    d.total_row_count_loaded != 0
ORDER BY
    COALESCE(a.activity_date, p.pipeline_date, d.load_date);












SELECT meta_pipelines.pipeline_name, meta_pipelines.status, AVG(meta_copyactivity.rows_read) AS avg_rows_read, AVG(meta_copyactivity.data_read) AS avg_data_read, AVG(meta_copyactivity.rows_written) AS avg_rows_written, AVG(meta_copyactivity.data_written) AS avg_data_written FROM meta_pipelines INNER JOIN meta_copyactivity ON meta_pipelines.run_id = meta_copyactivity.parent_runid GROUP BY meta_pipelines.pipeline_name, meta_pipelines.status


WITH RankedData AS (
    SELECT
        mp.run_id AS pipeline_run_id,
        mp.start_time AS pipeline_start_time,
        mp.end_time AS pipeline_end_time,
        mc.activity_runid AS activity_run_id,
        mc.activity_name AS activity_name,
        dl.table_name AS loaded_table,
        SUM(dl.row_count) AS total_row_count_loaded,
        DENSE_RANK() OVER (ORDER BY SUM(dl.row_count) DESC) AS rank
    FROM
        meta_pipelines mp
    JOIN
        meta_copyactivity mc ON mp.run_id = mc.parent_runid
    LEFT JOIN
        dataload dl ON mc.activity_runid = dl.parent_runid
    GROUP BY
        mp.run_id,
        mp.start_time,
        mp.end_time,
        mc.activity_runid,
        mc.activity_name,
        dl.table_name
    HAVING
        SUM(dl.row_count) != 0
)
SELECT *
FROM RankedData
ORDER BY rank, pipeline_run_id, activity_run_id, loaded_table;
















from unidecode import unidecode

def remove_diacritics_and_replace(input_str):
    return unidecode(input_str)

input_string = "Joaé Ëx Âpple ñice Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)














import pyspark.sql.functions as F

def remove_diacritics_udf(input_str):
    return F.udf(lambda x: unidecode(x))(input_str)

schema = StructType([StructField('text', StringType(), True)])

spark_df = spark.createDataFrame(data=[['Joaé Ëxample Æpple ñicely Þorn']], schema=schema)

spark_df = spark_df.withColumn('text_no_diacritics', remove_diacritics_udf(spark_df['text']))

print(spark_df.show())














schema = StructType([StructField('text', StringType(), True)])

spark_df = spark.createDataFrame(data=[['Joaé Ëxample Æpple ñicely Þorn']], schema=schema)















import pyspark.sql.functions as F

def remove_diacritics_udf(input_str):
    return F.udf(lambda x: unidecode(x))(input_str)

spark_df = spark.createDataFrame({'text': ['Joaé Ëxample Æpple ñicely Þorn']})

spark_df = spark_df.withColumn('text_no_diacritics', remove_diacritics_udf(spark_df['text']))

print(spark_df.show())












from unidecode import unidecode

def remove_diacritics_and_replace(input_str):
    return unidecode(input_str)

input_string = "Joaé Ëxample Âpple ñicely Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)












import unicodedata

def remove_diacritics_and_replace(input_str):
    normalized_str = unicodedata.normalize('NFD', input_str)
    output_str = ''
    
    replacement_dict = {
        unicodedata.normalize('NFD', 'À'): 'A',
        unicodedata.normalize('NFD', 'Á'): 'A',
        unicodedata.normalize('NFD', 'Â'): 'A',
        unicodedata.normalize('NFD', 'Ã'): 'A',
        unicodedata.normalize('NFD', 'Ä'): 'A',
        unicodedata.normalize('NFD', 'Å'): 'A',
        unicodedata.normalize('NFD', 'à'): 'a',
        unicodedata.normalize('NFD', 'á'): 'a',
        unicodedata.normalize('NFD', 'â'): 'a',
        unicodedata.normalize('NFD', 'ã'): 'a',
        unicodedata.normalize('NFD', 'ä'): 'a',
        unicodedata.normalize('NFD', 'å'): 'a',
        unicodedata.normalize('NFD', 'È'): 'E',
        unicodedata.normalize('NFD', 'É'): 'E',
        # ... and so on for other characters in the dictionary
    }
    
    for c in normalized_str:
        output_str += replacement_dict.get(c, c)
    
    return output_str

input_string = "Joaé Ëxample Âpple ñicely Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)






















def remove_diacritics_and_replace(input_str):
    normalized_str = unicodedata.normalize('NFD', input_str)
    output_str = ''
    
    replacement_dict = {
        'À': 'A', 'Á': 'A', 'Â': 'A', 'Ã': 'A', 'Ä': 'A', 'Å': 'A',
        'à': 'a', 'á': 'a', 'â': 'a', 'ã': 'a', 'ä': 'a', 'å': 'a',
        'È': 'E', 'É': 'E', 'Ê': 'E', 'Ë': 'E',
        'è': 'e', 'é': 'e', 'ê': 'e', 'ë': 'e',
        'Ì': 'I', 'Í': 'I', 'Î': 'I', 'Ï': 'I',
        'ì': 'i', 'í': 'i', 'î': 'i', 'ï': 'i',
        'Ò': 'O', 'Ó': 'O', 'Ô': 'O', 'Õ': 'O', 'Ö': 'O', 'Ø': 'O',
        'ò': 'o', 'ó': 'o', 'ô': 'o', 'õ': 'o', 'ö': 'o', 'ø': 'o',
        'Ù': 'U', 'Ú': 'U', 'Û': 'U', 'Ü': 'U',
        'ù': 'u', 'ú': 'u', 'û': 'u', 'ü': 'u',
        'Ý': 'Y', 'ý': 'y', 'ÿ': 'y',
        'Ç': 'C', 'ç': 'c',
        'Ñ': 'N', 'ñ': 'n',
        'Æ': 'AE', 'æ': 'ae',
        'Þ': 'Th', 'þ': 'th', 'Ð': 'D', 'ð': 'd',  # Icelandic characters
        'ß': 'ss',
        'œ': 'oe', 'Œ': 'OE',
        'Ā': 'A', 'ā': 'a', 'Ă': 'A', 'ă': 'a', 'Ą': 'A', 'ą': 'a',  # Latin extended-A
        'Ć': 'C', 'ć': 'c', 'Ĉ': 'C', 'ĉ': 'c', 'Ċ': 'C', 'ċ': 'c',
    }
    
    for c in normalized_str:
        output_str += replacement_dict.get(c, c)
    
    return output_str

input_string = "Joaé Ëxample Âpple ñicely Þorn"
modified_string = remove_diacritics_and_replace(input_string)
print(modified_string)

